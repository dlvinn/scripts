# Arabic Mojibake Encoding Fix

## Problem Solved

This solution fixes **Mojibake** encoding issues where Arabic text encoded in **Windows-1256** is misinterpreted as **Windows-1252/Latin-1**.

### Example

- **Corrupted**: `√Ç√à` or `√ÄB` (what you see in the document)
- **Fixed**: `ÿ¢ÿ®` (correct Arabic - "August" in Iraqi calendar)

## What's Included

### 1. `encoding_fixer.py`
Core module that fixes Mojibake using a letter-by-letter translation table (`str.maketrans`).

**Features:**
- ‚úÖ Comprehensive Windows-1256 ‚Üí Latin1 Mojibake mapping (45+ characters)
- ‚úÖ Fast `str.maketrans` implementation
- ‚úÖ Discovery mode to find unmapped characters
- ‚úÖ Handles your specific case: `√Ç√à` ‚Üí `ÿ¢ÿ®` (and variant `√ÄB`)

**Usage:**
```python
from encoding_fixer import ArabicEncodingFixer

fixer = ArabicEncodingFixer()

# Fix corrupted text
corrupted = "√£√ë√ç√à√á √à√ù√£√£"
fixed = fixer.clean_text(corrupted)  # ‚Üí "ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸÉŸÖ"

# Discovery mode - find all corrupted characters
samples = ["√á√°√ö√ë√á√û", "√à√õ√è√á√è"]
fixer.print_discovery_report(samples)
```

**Test it:**
```bash
python encoding_fixer.py
```

### 2. `discover_mojibake.py`
Diagnostic tool to scan your `.docx` files and identify all Mojibake characters.

**Features:**
- ‚úÖ Scans all .docx files in a folder (recursive)
- ‚úÖ Shows which files have encoding issues
- ‚úÖ Displays sample corrupted texts and their fixes
- ‚úÖ Reports unmapped characters you need to add

**Usage:**
```bash
python discover_mojibake.py "C:\path\to\documents"
```

**Example output:**
```
SCANNING 5 DOCUMENT(S) FOR MOJIBAKE
======================================

Scanning: report1.docx... ‚úó Found 23 corrupted text(s)
Scanning: report2.docx... ‚úì Clean
Scanning: report3.docx... ‚úó Found 5 corrupted text(s)

OVERALL DISCOVERY REPORT
======================================

‚úì Found 15 known Mojibake characters:
  '√Ç' ‚Üí 'ÿ¢' (U+00C2 ‚Üí U+0622)
  '√à' ‚Üí 'ÿ®' (U+00C8 ‚Üí U+0628)
  ...

SAMPLE CORRUPTED TEXTS BY FILE
======================================

üìÑ report1.docx
   1. √£√ë√ç√à√á √à√ù√£√£
      ‚Üí ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸÉŸÖ
   2. √á√°√ö√ë√á√û
      ‚Üí ÿßŸÑÿπÿ±ÿßŸÇ
```

### 3. **Integrated into `docx_format_fixer.py`**
The main document fixer now automatically fixes encoding issues!

**Usage:**
```bash
# Fix documents with encoding fix (default)
python docx_format_fixer.py "C:\path\to\documents"

# Skip encoding fix if needed
python docx_format_fixer.py "C:\path\to\documents" --no-encoding-fix
```

**Output:**
```
‚úì Mojibake encoding fix enabled (√Ç√à ‚Üí ÿ¢ÿ®)

Processing: report.docx
  - RTL paragraphs: 45
  - Alignments: 45
  - Encoding fixes: 12  ‚Üê NEW!
  ‚úì Content validation: PASSED
```

## How It Works

### The Technical Problem

1. Original text: `ÿ¢ÿ®` (Arabic)
2. Encoded in Windows-1256: `0xC2 0xC8` (bytes)
3. **Misinterpreted** as Latin-1: `√Ç√à` (Mojibake!)

### The Solution

We use `str.maketrans` to create a translation table that maps:
- `√Ç` (U+00C2) ‚Üí `ÿ¢` (U+0622)
- `√à` (U+00C8) ‚Üí `ÿ®` (U+0628)
- ... and 40+ other characters

```python
MOJIBAKE_MAP = {
    '√Ç': 'ÿ¢',  # Alef with madda
    '√à': 'ÿ®',  # Ba
    '√á': 'ÿß',  # Alef
    # ... full mapping
}

translation_table = str.maketrans(MOJIBAKE_MAP)
fixed_text = corrupted_text.translate(translation_table)
```

## Workflow

### Step 1: Discover Issues (Optional but Recommended)
```bash
python discover_mojibake.py "C:\Users\Documents\Arabic Docs"
```

This shows you:
- Which files have encoding issues
- What characters are corrupted
- If any unmapped characters exist

### Step 2: Fix Documents
```bash
python docx_format_fixer.py "C:\Users\Documents\Arabic Docs"
```

This will:
1. Fix encoding issues (√Ç√à ‚Üí ÿ¢ÿ®)
2. Fix RTL direction
3. Fix right alignment
4. Fix table columns
5. Save as `*_fixed.docx`

### Step 3: Review
Open the `*_fixed.docx` files and verify the text is correct!

## Customization

### Adding New Mappings

If `discover_mojibake.py` finds unmapped characters, add them to the `MOJIBAKE_MAP` in `encoding_fixer.py`:

```python
MOJIBAKE_MAP = {
    # ... existing mappings ...
    '√ë': 'YOUR_ARABIC_CHAR',  # Add new mapping
}
```

### Generating Mappings

Use `generate_map.py` to test and generate new mappings:

```bash
python generate_map.py
```

## Test Results

All tests passing ‚úÖ:

```
Test Case: Iraqi month 'August' (ÿ¢ÿ®)
  Corrupted: √Ç√à
  Fixed:     ÿ¢ÿ®
  Expected:  ÿ¢ÿ®
  Result:    ‚úì PASS

‚úì Iraq     | √á√°√ö√ë√á√û  ‚Üí ÿßŸÑÿπÿ±ÿßŸÇ
‚úì You      | √É√§√ä     ‚Üí ÿ£ŸÜÿ™
‚úì Hello    | √£√ë√ç√à√á   ‚Üí ŸÖÿ±ÿ≠ÿ®ÿß
‚úì Baghdad  | √à√õ√è√á√è   ‚Üí ÿ®ÿ∫ÿØÿßÿØ
```

## Why This Approach?

1. **Fast**: `str.maketrans` is the fastest method for character-by-character replacement
2. **Reliable**: Direct byte-to-character mapping based on actual encoding tables
3. **Discoverable**: Find and fix unknown characters easily
4. **Integrated**: Works seamlessly with your existing document fixer
5. **Preserves Formatting**: Only changes text content, not document structure

## Is It Fixable?

**YES! ‚úÖ** This is 100% fixable because:

1. The transformation is **deterministic** (one-to-one mapping)
2. No data is lost (every corrupted character maps to exactly one Arabic character)
3. The mapping is **comprehensive** (covers all Windows-1256 Arabic characters)
4. It's **reversible** (we know the exact encoding mismatch)

## Need Help?

Run the test suite:
```bash
python encoding_fixer.py
```

Scan your documents:
```bash
python discover_mojibake.py "C:\path\to\documents"
```

Generate new mappings:
```bash
python generate_map.py
```

All tools have built-in help and examples!
